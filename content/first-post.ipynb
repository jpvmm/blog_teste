{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/joao/virtualenvs/talk-docker/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/joao/virtualenvs/talk-docker/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/joao/virtualenvs/talk-docker/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import string\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       10658\n",
       "Text             10658\n",
       "Classificacao    10658\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lê os dados do disco\n",
    "data = pd.read_csv('./dataset_twitter.csv', encoding='utf-8', sep=',')\n",
    "\n",
    "#Tira as linhas com NaN\n",
    "data = data.dropna() \n",
    "\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       False\n",
       "Text             False\n",
       "Classificacao    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()\n",
    "#A coluna text possui nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10658, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Classificacao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Text, Classificacao]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rt ja pensou presentear sua tia querida no amigo secreto com um desses ecoracao https',\n",
       "       'ja pensou presentear sua tia querida no amigo secreto com um desses https t co ezbwznwwv',\n",
       "       'rt nam hoje no shopping com o aniversariante jimin i me desmarca disso https t co gpr',\n",
       "       ..., 'meu nene ta bravo',\n",
       "       'rt o discurso de na saude e na doenca e td mentira na chance cai fora',\n",
       "       'parece https t co qrzjfdll'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = data['Text'].values\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positivo', 'Positivo', 'Positivo', ..., 'Negativo', 'Negativo',\n",
       "       'Negativo'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['Classificacao'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    ''' Clean documents '''\n",
    "    \n",
    "    tokens = doc.split()\n",
    "    \n",
    "    #Regex para extraçao de links\n",
    "    re_http = re.compile('http\\S+')\n",
    "    \n",
    "    tokens = [re_http.sub(' ', w) for w in tokens]\n",
    "\n",
    "    #Regex para filtro de caracteres\n",
    "    re_puc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    #Remove pontucao\n",
    "    tokens = [re_puc.sub('', w ) for w in tokens]\n",
    "\n",
    "    #Remove tokens que nao sao alfabeticos\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    #Filtra stopwords\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1] #deixa tudo minusculo \n",
    "\n",
    "    tokens = ' '.join(tokens)\n",
    "\n",
    "    #tokens = ''.join(tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [clean_doc(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt ja pensou presentear tia querida amigo secreto desses ecoracao Positivo\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acredito semana stranger things saudade filhos --> Positivo\n"
     ]
    }
   ],
   "source": [
    "#Separando os dados em teste e treinamento\n",
    "x_train, x_test, y_train, y_test = train_test_split(tokens, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print(x_train[100],'-->', y_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando unigrams e bigrams\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "counts = ngram_vectorizer.fit_transform(x_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adoram',\n",
       " 'adoram fazer',\n",
       " 'aha',\n",
       " 'aha morram',\n",
       " 'aleatorias',\n",
       " 'aleatorias uns',\n",
       " 'amigas',\n",
       " 'amigas sempre',\n",
       " 'amor',\n",
       " 'amor vida',\n",
       " 'andar',\n",
       " 'andar moro',\n",
       " 'anos',\n",
       " 'anos colocaram',\n",
       " 'barata',\n",
       " 'barata segunda',\n",
       " 'bendito',\n",
       " 'bendito onde',\n",
       " 'bpudr',\n",
       " 'brasil',\n",
       " 'chamado',\n",
       " 'chamado amigas',\n",
       " 'checagem',\n",
       " 'checagem limpeza',\n",
       " 'co',\n",
       " 'co bpudr',\n",
       " 'co hcbpb',\n",
       " 'co jomfury',\n",
       " 'co khmeplfra',\n",
       " 'co rsrgesrlzd',\n",
       " 'co tzturpsdy',\n",
       " 'co wnrcnef',\n",
       " 'colocaram',\n",
       " 'colocaram grupo',\n",
       " 'compartilhando',\n",
       " 'compartilhando resultado',\n",
       " 'corredor',\n",
       " 'corredor andar',\n",
       " 'deu',\n",
       " 'deu presente',\n",
       " 'echorando',\n",
       " 'est',\n",
       " 'est co',\n",
       " 'facebook',\n",
       " 'facebook chamado',\n",
       " 'fazer',\n",
       " 'fazer trouxa',\n",
       " 'feira',\n",
       " 'feira co',\n",
       " 'fico',\n",
       " 'fico saber',\n",
       " 'grupo',\n",
       " 'grupo facebook',\n",
       " 'guardar',\n",
       " 'guardar bendito',\n",
       " 'hcbpb',\n",
       " 'hcbpb co',\n",
       " 'implementei',\n",
       " 'implementei sistema',\n",
       " 'jomfury',\n",
       " 'khmeplfra',\n",
       " 'limpeza',\n",
       " 'limpeza corredor',\n",
       " 'mandamentos',\n",
       " 'mandamentos guardar',\n",
       " 'matando',\n",
       " 'matando saudade',\n",
       " 'matei',\n",
       " 'matei barata',\n",
       " 'meninas',\n",
       " 'meninas aleatorias',\n",
       " 'mercadear',\n",
       " 'mercadear co',\n",
       " 'mim',\n",
       " 'mim co',\n",
       " 'moro',\n",
       " 'moro matei',\n",
       " 'morram',\n",
       " 'obedecer',\n",
       " 'obedecer voz',\n",
       " 'ocorrido',\n",
       " 'ocorrido saber',\n",
       " 'onde',\n",
       " 'onde passar',\n",
       " 'ouvir',\n",
       " 'ouvir mandamentos',\n",
       " 'passar',\n",
       " 'passar echorando',\n",
       " 'pessoal',\n",
       " 'pessoal co',\n",
       " 'pessoas',\n",
       " 'pessoas adoram',\n",
       " 'pra',\n",
       " 'pra mim',\n",
       " 'presente',\n",
       " 'presente co',\n",
       " 'resultado',\n",
       " 'resultado pessoal',\n",
       " 'rsrgesrlzd',\n",
       " 'rt',\n",
       " 'rt voce',\n",
       " 'saber',\n",
       " 'saber est',\n",
       " 'saber ocorrido',\n",
       " 'saudade',\n",
       " 'saudade mercadear',\n",
       " 'segunda',\n",
       " 'segunda feira',\n",
       " 'sei',\n",
       " 'sei obedecer',\n",
       " 'sempre',\n",
       " 'sempre aha',\n",
       " 'sistema',\n",
       " 'sistema checagem',\n",
       " 'surpresa',\n",
       " 'surpresa pra',\n",
       " 'trouxa',\n",
       " 'tv',\n",
       " 'tv fico',\n",
       " 'tzturpsdy',\n",
       " 'umas',\n",
       " 'umas meninas',\n",
       " 'uns',\n",
       " 'uns anos',\n",
       " 'vida',\n",
       " 'vida deu',\n",
       " 'voce',\n",
       " 'voce amor',\n",
       " 'voz',\n",
       " 'voz ouvir',\n",
       " 'wnrcnef']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construindo a Pipeline com Sklearn\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC())])\n",
    "    #('clf', GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros que serão testados durante o grid_search\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((2, 3), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C': ([1e3, 5e3]),\n",
    "    #'clf__n_estimators':([50, 100, 150]),\n",
    "    #'clf__learning_rate':([0.5, 1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  48 out of  48 | elapsed:   19.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=3,\n",
       "       param_grid={'vect__ngram_range': ((2, 3), (1, 2)), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__C': [1000.0, 5000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...ax_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negativo       0.88      0.84      0.86      1489\n",
      "   Positivo       0.81      0.85      0.83      1176\n",
      "\n",
      "avg / total       0.85      0.85      0.85      2665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8457786116322702"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classificador.pkl']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SALVANDO O MODELO PARA UM ARQUIVO PICKLE\n",
    "joblib.dump(grid_search, 'classificador.pkl', compress = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = joblib.load('classificador.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negativo'], dtype=object)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.predict(['O Lula é um bosta #ladrão'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
